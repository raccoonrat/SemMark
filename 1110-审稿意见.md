### 《大模型文本语义水印研究综述》审稿意见

#### **一、创新性评价**

本文在**系统性框架构建**和**争议点深度挖掘**上具有显著创新，主要体现在以下方面：

**三维分类框架的提出**：  
基于嵌入维度（token/句子/语义级）、检测方式（统计检验/神经网络/后处理）和威胁模型（白盒/黑盒/公开检测）的三维分类框架，突破了传统按“嵌入层级”单一维度分类的局限，为领域提供了清晰的技术路线图。该框架不仅覆盖了方法类型，还关联了应用场景（如白盒适用于模型提供方，黑盒适用于第三方检测），体现了“方法-场景-安全”的综合视角。

**定量筛选标准的创新性**：  
提出“方法原创性、场域影响力、可复用度、实验透明度”四维度量化评估体系，并设定明确的阈值（如原创性≥7分、引用数≥20次），解决了传统综述中论文选择主观性强的问题。这种“量化+阈值”的筛选机制，提升了核心论文的代表性和结论的可信度。

**争议点分析的深度**：  
识别8个关键争议点（如检测样本量门槛、多比特可靠性、跨语种一致性等），并量化方法间差异（指标波动>15%），揭示了领域内未达成共识的核心矛盾。例如，对“多比特水印是否必然牺牲鲁棒性”的辨析，打破了传统“容量-鲁棒性-质量”三难问题的固有认知，为后续研究提供了新方向。

**改进建议**：

* 三维分类框架可进一步扩展“任务适配性”维度（如对话生成、长文本摘要等不同任务对水印的需求差异），增强框架的实践指导意义。
* 筛选标准中“场域影响力”仅以会议期刊权重和引用数衡量，可补充“工业落地案例”（如SynthID-Text的Gemini部署）作为辅助指标，更全面评估方法的实际价值。

#### **二、论证逻辑性评价**

论文整体结构清晰（引言→方法论→分类框架→方法分析→攻防→定量→争议→未来方向），逻辑链条完整，但部分环节存在优化空间：

**方法论部分的严谨性**：

筛选标准中“方法原创性”的评分（如“新嵌入机制”“新检测算法”）未明确具体评分细则，可能导致主观偏差。例如，KGW的“统计检验”与UPV的“神经网络检测”如何量化评分差异，需更客观的指标（如算法复杂度、理论突破程度）。 “专家评审”环节未说明专家背景（如是否涵盖工业界、安全领域研究者）和评审流程（如是否采用盲审、一致性检验机制），可能影响筛选结果的公信力。

**争议点分析的逻辑深度**：  
争议点（如“无偏水印的真实鲁棒性”）提到了“多轮生成/低熵段累积漂移”的现象，但未深入分析其根本原因（如无偏方法对输出分布的严格约束是否限制了水印嵌入的灵活性）。建议结合具体案例（如Unbiased方法在低熵文本中的检测失败案例）增强论证力度。

**攻防动态演进的逻辑衔接**：  
将攻防分为四个阶段（2021-2022起步、2023发展、2024成熟、2025前沿），但阶段间的“驱动因素”分析不足。例如，2024年语义级方法兴起是否直接源于2023年释义攻击的暴露？建议补充“攻击-防御”的因果链条，揭示技术演进的内在逻辑。

**改进建议**：

* 在方法论中补充“评分细则示例”（如原创性评分可参考“是否提出新理论模型”“是否解决已知瓶颈”等二级指标），并公开专家评审的反馈意见，提升透明度。
* 争议点分析可增加“案例对比”（如对比KGW与SemStamp在相同释义攻击下的表现差异），通过具体数据支撑论点。

#### **三、数据可靠性评价**

论文数据来源广泛（Google Scholar、arXiv、ACL Anthology等），但部分数据的可比性和完整性存在疑问：

**性能指标对比的基准统一性**：  
表1（主要方法性能指标对比）中，不同方法的“AUC”“所需Token数”可能基于不同实验设置（如数据集、攻击类型、水印强度）。例如，KGW的AUC 0.85-0.90基于特定释义攻击，而SemStamp的AUC 0.90-0.95是否包含相同攻击？未说明“统一水印强度”的设定，可能导致对比不公平。

**争议点数据的代表性**：

“多比特水印可靠性”中，Provably Robust Multi-bit的97.6%匹配率 vs SOTA 49.2%，未说明SOTA的具体方法（是传统多比特还是单比特），且未提供样本量信息（如测试文本数量），难以判断结果的统计显著性。 “跨语种一致性”中，翻译攻击使AUC从0.95降至0.67，但未明确翻译方向（如英译中 vs 中译英）和翻译模型（如Google Translate vs 专用神经翻译器），不同方向的攻击强度可能存在差异。

**引用数据的时效性**：  
引用数统计截至“2025年1月”，但部分论文（如2025年USENIX Security的Provably Robust Multi-bit）可能尚未被广泛引用，高引用数可能集中于早期经典工作（如KGW），导致对新兴方法的评估偏保守。

**改进建议**：

* 采用“统一基准”（如WaterBench框架）重新整理性能数据，明确实验设置（数据集、攻击类型、水印强度参数），并在表格中标注数据来源。
* 争议点数据需补充“实验细节”（如样本量、攻击工具版本），并说明“SOTA”的具体指代对象，避免模糊表述。
* 对2025年的论文，可补充“预印本引用数”或“领域专家关注度调研”，平衡时效性偏差。

#### **四、结论合理性评价**

论文结论基于对30篇核心论文的系统分析，主要观点（语义级方法鲁棒性优、多比特可兼顾容量与鲁棒性等）具有较强说服力，但部分结论的普适性有待商榷：

**语义级方法的“计算开销”结论**：  
结论指出语义级方法“计算开销大（∼1.5-2.0×）”，但未区分“嵌入开销”和“检测开销”。例如，SemStamp的嵌入开销高，但其检测仅需统计检验（O(n)），而UPV的检测需神经网络（O(n×m)），综合开销可能低于某些语义级方法。建议细化“计算开销”的维度分析。

**“强水印不可能性”的工程折中结论**：  
结论认为“强水印不可能性理论不等于工程不可行”，但未给出“工程可行”的具体标准（如可接受的误报率、检测成本）。例如，Watermark Stealing攻击的成本<$50，若防御方案的成本超过$100，是否仍具工程价值？需补充成本效益分析。

**跨语种防御的“有效性”结论**：  
X-SIR防御将跨语种AUC提升至∼0.87，但仍低于单语种性能（∼0.95），结论中“仍需进一步改进”的表述较模糊。建议明确“改进目标”（如是否需达到单语种AUC>0.9），或提出跨语种防御的“最低可接受标准”。

**改进建议**：

* 结论中增加“场景化建议”（如“语义级方法适用于对鲁棒性要求高的长文本生成，token级方法适用于实时性要求高的短对话场景”），避免“一刀切”的结论。
* 对“强水印不可能性”的工程折中，补充“现实威胁模型下的可行性边界”（如“在允许5%误报率、检测成本<$10的场景下，多比特水印可实现有效溯源”）。

#### **五、其他改进建议**

**术语统一性**：  
论文中“无偏（Unbiased）水印”与“有偏（Biased）水印”的定义需进一步明确（如“无偏”是否严格指“输出分布不变”），避免与“语义级水印”等概念混淆。

**图表优化**：  
表2（争议点总结表）中“创新机会”评分（1-5星）缺乏评分依据，建议补充评分标准（如“基于当前技术瓶颈和工业需求”）；可增加“攻防博弈趋势图”，直观展示各阶段攻击与防御的强度变化。

**未来方向的细化**：  
“短文本场景优化”方向可具体到“≤50 tokens的超短文本”（如社交媒体评论），并提出“轻量级检测算法”（如基于关键词匹配的快速检测）的可行性路径。

#### **六、总体评价**

本文是近年来大模型文本语义水印领域**最系统、最深入的综述之一**，其三维分类框架、定量筛选标准和争议点分析填补了领域空白，为研究人员提供了清晰的技术路线图。尽管在数据可靠性、论证严谨性等方面存在改进空间，但整体创新性强、逻辑清晰、结论有价值。**推荐在修改后发表**，修改重点应放在：

* 统一实验基准，提升数据可比性；
* 细化评分标准和争议点分析依据；
* 补充场景化结论和未来方向的细化路径。
